{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scott Ming 2017-05-30 \n",
      "\n",
      "CPython 3.6.0\n",
      "IPython 6.0.0\n",
      "\n",
      "numpy 1.12.1\n",
      "sklearn 0.18.1\n",
      "tensorflow 1.0.1\n",
      "\n",
      "compiler   : GCC 4.9.2\n",
      "system     : Linux\n",
      "release    : 3.16.0-4-amd64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Scott Ming' -v -m -d -p numpy,sklearn,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 让机器学会做加法：\n",
    "\n",
    "- 例如:\n",
    "    + 输入 \"1+12\", 输出\"13\"\n",
    "    + 输入 \"324+176\", 输出\"500\"\n",
    "    + 输入 \"154+33\", 输出\"187\"\n",
    "- 注意输入和输出都是`字符串`类型，我们的目的是让机器在不知道加法法则的情况下学会加法运算。\n",
    "- 为减小难度，输入字符串在加号左右的数字限定为三位数之内的正整数\n",
    "- 训练集、验证集和测试集数据由自己生成\n",
    "- 文档中需要说明\n",
    "    + 数据的生成过程\n",
    "    + 模型是如何设计的\n",
    "    + 超参数是如何选择的\n",
    "    + 目标函数和优化方法如何选择\n",
    "    + 模型在测试集上的准确率是多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.contrib.legacy_seq2seq import (basic_rnn_seq2seq, \n",
    "                                                embedding_rnn_seq2seq, sequence_loss)\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_texts(low, high, size):\n",
    "    x1 = np.random.randint(low=low, high=high, size=size)\n",
    "    x2 = np.random.randint(low=low, high=high, size=size)\n",
    "    y = x1 + x2\n",
    "    # Concat x1, x2\n",
    "    x_texts = [str(left) + '+' + str(right) for left, right in zip(x1, x2)]\n",
    "    y_texts = [str(i) for i in y]\n",
    "    return x_texts, y_texts\n",
    "\n",
    "\n",
    "def build_dict(words):\n",
    "    word_counts = Counter(words).most_common()\n",
    "    count = [['<UNK>', -1]]\n",
    "    count.extend(word_counts)\n",
    "    vocab2ix = {key: ix for ix, (key, _) in enumerate(count)}\n",
    "    vocab2ix['<GO>'] = max(vocab2ix.values()) + 1\n",
    "    vocab2ix['<EOS>'] = max(vocab2ix.values()) + 1\n",
    "    ix2vocab = {value: key for key, value in vocab2ix.items()}\n",
    "    return vocab2ix, ix2vocab\n",
    "\n",
    "\n",
    "def append_go_eos(nested_list):\n",
    "    nested_list = [deque(list_) for list_ in nested_list]\n",
    "    for deque_ in nested_list:\n",
    "        deque_.appendleft('<GO>')\n",
    "        deque_.append('<EOS>')\n",
    "    nested_list = [list(deque_) for deque_ in nested_list]\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_pad(encoder_inputs, encoder_length, decoder_inputs, decoder_length, vocab, pad_symbol='<UNK>'):\n",
    "    \"\"\"\n",
    "    - encoder_input: A nested list of symbol str for encoding, length: batch_size\n",
    "    - encoder_length: max length of encoder input\n",
    "    - decoder_input: A nested list of symbol str for decoding, length: batch_size\n",
    "    - decoder_length: max length of decoder input\n",
    "    - vocab: vocabulary index, symbol (str) -> index (int)\n",
    "    \n",
    "    Example: \n",
    "    [\"hello\", \"world\"] -> [\"hi\", \"<EOS>\"]\n",
    "    [\"cover\", \"me\"] -> [\"roger\", \"<EOS>\"]\n",
    "    \n",
    "    seq2seq_pad([['hello', 'world'], ['cover', 'me']], 4, [['hi', '<EOS>'], ['roger', '<EOS>']], 4, vocab)\n",
    "    \n",
    "    Assume that index of \"<PAD>\" is 0\n",
    "\n",
    "    Output:\n",
    "    [[0, 0, <index of 'hello'>, <index of 'world'>], [0, 0, <index of 'cover'>, <index of 'me'>]],\n",
    "    [[<index of 'hi'>, <index of 'EOS'>, 0, 0], [<index of 'roger'>, <index of 'EOS'>, 0, 0]]\n",
    "    \"\"\"\n",
    "    pad_index = vocab[pad_symbol]\n",
    "    def to_index(inputs, length, pad_from_start=True):\n",
    "        inputs_to_index = []\n",
    "        for cur_input in inputs:\n",
    "            cur_input_to_index = [pad_index] * length\n",
    "            l = len(cur_input)\n",
    "            if l < length:\n",
    "                if pad_from_start:\n",
    "                    cur_input_to_index[(length - l):] = [vocab[i] for i in cur_input]\n",
    "                else:\n",
    "                    cur_input_to_index[:l] = [vocab[i] for i in cur_input]\n",
    "            else:\n",
    "                cur_input_to_index = [vocab[i] for i in cur_input[:length]]\n",
    "            inputs_to_index.append(cur_input_to_index)    \n",
    "        return inputs_to_index\n",
    "    return to_index(encoder_inputs, encoder_length, True), to_index(decoder_inputs, decoder_length, False)\n",
    "\n",
    "\n",
    "def left_shift(decoder_inputs, pad_idx):\n",
    "    # for generating targets\n",
    "    return [list(input_[1:]) + [pad_idx] for input_ in decoder_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_texts, y_texts = build_texts(low=0, high=999, size=500000)\n",
    "x_words = [list(sequence) for sequence in x_texts]\n",
    "y_words = [list(sequence) for sequence in y_texts]\n",
    "data = itertools.chain(x_words, y_words)  # Concat all data\n",
    "words = itertools.chain.from_iterable(data)  # Flat data\n",
    "vocab2ix, ix2vocab = build_dict(words)\n",
    "y_words = append_go_eos(y_words)  # 方便排列，<GO>/<EOS> 在创建字典之后添加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分割训练数据和验证数据\n",
    "dataset = np.c_[x_words, y_words]  # 按列拼接\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "x_train, y_train = train_set[:, 0], train_set[:, 1]\n",
    "x_test, y_test = test_set[:, 0], test_set[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "encoder_length = max(len(x) for x in x_train)\n",
    "decoder_length = max(len(x) for x in y_train)\n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(128)\n",
    "num_encoder_symbols = len(vocab2ix)\n",
    "num_decoder_symbols = len(vocab2ix)\n",
    "train_dataset_size = y_train.shape[0]\n",
    "batch_size = 50\n",
    "embedding_size = 128\n",
    "epochs = 20001\n",
    "print_loss_every = 1000\n",
    "learning_rate = 0.0003\n",
    "\n",
    "encoder_placeholders = [tf.placeholder(tf.int32, shape=[None],\n",
    "                                       name=\"encoder_%d\" % i) for i in range(encoder_length)]\n",
    "decoder_placeholders = [tf.placeholder(tf.int32, shape=[None],\n",
    "                                       name=\"decoder_%d\" % i) for i in range(decoder_length)]\n",
    "target_placeholders = [tf.placeholder(tf.int32, shape=[None],\n",
    "                                      name=\"target_%d\" % i) for i in range(decoder_length)]\n",
    "target_weights_placeholders = [tf.placeholder(tf.float32, shape=[None],\n",
    "                                              name=\"decoder_weight_%d\" % i) for i in range(decoder_length)]\n",
    "outputs, states = embedding_rnn_seq2seq(encoder_placeholders, decoder_placeholders, cell,\n",
    "                                        num_encoder_symbols, num_decoder_symbols,\n",
    "                                        embedding_size, output_projection=None,\n",
    "                                        feed_previous=False)\n",
    "\n",
    "loss = sequence_loss(outputs, target_placeholders, target_weights_placeholders)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(encoder_inputs, decoder_inputs):\n",
    "    encoder_inputs = list(zip(*encoder_inputs))\n",
    "    target_inputs = list(zip(*left_shift(decoder_inputs, vocab2ix['<UNK>'])))\n",
    "    decoder_inputs = list(zip(*decoder_inputs))\n",
    "    \n",
    "    feed_dict = dict()\n",
    "    # Prepare input data    \n",
    "    for (i, placeholder) in enumerate(encoder_placeholders):\n",
    "        # 这里用 placeholder 或者 placeholder.name 都可以\n",
    "        feed_dict[placeholder.name] = np.asarray(encoder_inputs[i], dtype=int)\n",
    "    for i in range(len(decoder_placeholders)):\n",
    "        feed_dict[decoder_placeholders[i].name] = np.asarray(decoder_inputs[i], dtype=int)\n",
    "        feed_dict[target_placeholders[i].name] = np.asarray(target_inputs[i], dtype=int)        \n",
    "        # 这里使用 weights 把 <PAD> 的损失屏蔽了\n",
    "        feed_dict[target_weights_placeholders[i].name] = np.asarray(\n",
    "            [float(idx != vocab2ix['<UNK>'])\n",
    "             for idx in target_inputs[i]], dtype=float)\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def get_output_words(outputs_list):\n",
    "    test_output_array = np.asarray(outputs_list).T\n",
    "    # 将获取的最高概率 numbers 转为 字符串\n",
    "    test_output_list = [[ix2vocab[idx] for idx in sublist]\n",
    "                                       for sublist in test_output_array]\n",
    "    # 删除 <EOS> 之后的词\n",
    "    test_output_list = [itertools.takewhile(lambda x: x != '<EOS>', sublist)\n",
    "                        for sublist in test_output_list]\n",
    "    # 把解码后的字符串拼接成句子\n",
    "    test_output_list = [''.join(i) for i in test_output_list]\n",
    "    return test_output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pad data\n",
    "train_encoders, train_decoders = seq2seq_pad(x_train, encoder_length, \n",
    "                                             y_train, decoder_length, vocab2ix)\n",
    "test_encoders, test_decoders = seq2seq_pad(x_test, encoder_length, \n",
    "                                           y_test, decoder_length, vocab2ix)\n",
    "# Set feed_dict\n",
    "train_feed_dict = get_feed_dict(train_encoders, train_decoders) \n",
    "test_feed_dict = get_feed_dict(test_encoders, test_decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 | Cost: 2.6877\n",
      "Epoch: 1000 | Cost: 1.3043\n",
      "Epoch: 2000 | Cost: 1.1225\n",
      "Epoch: 3000 | Cost: 0.9496\n",
      "Epoch: 4000 | Cost: 0.6554\n",
      "Epoch: 5000 | Cost: 0.5188\n",
      "Epoch: 6000 | Cost: 0.4227\n",
      "Epoch: 7000 | Cost: 0.4274\n",
      "Epoch: 8000 | Cost: 0.3680\n",
      "Epoch: 9000 | Cost: 0.3236\n",
      "Epoch: 10000 | Cost: 0.2814\n",
      "Epoch: 11000 | Cost: 0.2653\n",
      "Epoch: 12000 | Cost: 0.1887\n",
      "Epoch: 13000 | Cost: 0.1795\n",
      "Epoch: 14000 | Cost: 0.1269\n",
      "Epoch: 15000 | Cost: 0.1272\n",
      "Epoch: 16000 | Cost: 0.0919\n",
      "Epoch: 17000 | Cost: 0.0695\n",
      "Epoch: 18000 | Cost: 0.0583\n",
      "Epoch: 19000 | Cost: 0.0832\n",
      "Epoch: 20000 | Cost: 0.0574\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(epochs):\n",
    "    start = (i * batch_size) % train_dataset_size\n",
    "    end = min(start + batch_size, train_dataset_size)\n",
    "    feed_dict = get_feed_dict(train_encoders[start:end], train_decoders[start:end])\n",
    "    cost = sess.run(loss, feed_dict)\n",
    "    sess.run(train_step, feed_dict)\n",
    "    if i % print_loss_every == 0:\n",
    "        print(f'Epoch: {i:04d} | Cost: {cost:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test ACC: 0.944/0.942\n"
     ]
    }
   ],
   "source": [
    "with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n",
    "    outputs, states = embedding_rnn_seq2seq(encoder_placeholders, decoder_placeholders, \n",
    "                                            cell, num_encoder_symbols, num_decoder_symbols,\n",
    "                                            embedding_size, output_projection=None,\n",
    "                                            feed_previous=True)\n",
    "    \n",
    "    train_raw_outputs = [np.argmax(sess.run(o, train_feed_dict), axis=1) for o in outputs]\n",
    "    test_raw_outputs = [np.argmax(sess.run(o, test_feed_dict), axis=1) for o in outputs]\n",
    "    train_outputs = get_output_words(train_raw_outputs)\n",
    "    test_outputs  = get_output_words(test_raw_outputs)\n",
    "    train_targets = [''.join(i[1:-1]) for i in y_train]\n",
    "    test_targets  = [''.join(i[1:-1]) for i in y_test]\n",
    "    \n",
    "    # Compute acc\n",
    "    train_acc = accuracy_score(train_targets, train_outputs) \n",
    "    test_acc = accuracy_score(test_targets, test_outputs)\n",
    "    print(f'Train/Test ACC: {train_acc:.3f}/{test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 94+933\n",
      "Target: 1027\n",
      "Output: 1027\n",
      "-=--=--=--=--=-\n",
      "Input: 504+538\n",
      "Target: 1042\n",
      "Output: 1042\n",
      "-=--=--=--=--=-\n",
      "Input: 847+926\n",
      "Target: 1773\n",
      "Output: 1773\n",
      "-=--=--=--=--=-\n",
      "Input: 511+856\n",
      "Target: 1367\n",
      "Output: 1367\n",
      "-=--=--=--=--=-\n",
      "Input: 212+886\n",
      "Target: 1098\n",
      "Output: 1098\n",
      "-=--=--=--=--=-\n",
      "Input: 486+1\n",
      "Target: 487\n",
      "Output: 507\n",
      "-=--=--=--=--=-\n",
      "Input: 299+415\n",
      "Target: 714\n",
      "Output: 714\n",
      "-=--=--=--=--=-\n",
      "Input: 995+602\n",
      "Target: 1597\n",
      "Output: 1597\n",
      "-=--=--=--=--=-\n",
      "Input: 224+528\n",
      "Target: 752\n",
      "Output: 752\n",
      "-=--=--=--=--=-\n",
      "Input: 972+305\n",
      "Target: 1277\n",
      "Output: 1277\n",
      "-=--=--=--=--=-\n",
      "Input: 149+326\n",
      "Target: 475\n",
      "Output: 475\n",
      "-=--=--=--=--=-\n",
      "Input: 986+979\n",
      "Target: 1965\n",
      "Output: 1965\n",
      "-=--=--=--=--=-\n",
      "Input: 357+808\n",
      "Target: 1165\n",
      "Output: 1165\n",
      "-=--=--=--=--=-\n",
      "Input: 575+940\n",
      "Target: 1515\n",
      "Output: 1515\n",
      "-=--=--=--=--=-\n",
      "Input: 794+843\n",
      "Target: 1637\n",
      "Output: 1637\n",
      "-=--=--=--=--=-\n",
      "Input: 189+131\n",
      "Target: 320\n",
      "Output: 320\n",
      "-=--=--=--=--=-\n",
      "Input: 769+499\n",
      "Target: 1268\n",
      "Output: 1268\n",
      "-=--=--=--=--=-\n",
      "Input: 720+765\n",
      "Target: 1485\n",
      "Output: 1485\n",
      "-=--=--=--=--=-\n",
      "Input: 136+509\n",
      "Target: 645\n",
      "Output: 645\n",
      "-=--=--=--=--=-\n",
      "Input: 405+176\n",
      "Target: 581\n",
      "Output: 581\n",
      "-=--=--=--=--=-\n"
     ]
    }
   ],
   "source": [
    "# 随机选择 20 个测试样本看看具体结果\n",
    "r = np.random.permutation(y_test.shape[0])[:20]\n",
    "inputs_ = [''.join(i) for i in x_test[r]]\n",
    "targets_ = np.array(test_targets)[r]\n",
    "outputs_ = np.array(test_outputs)[r]\n",
    "for i, o, t in zip(inputs_, outputs_, targets_):\n",
    "    print('Input:', i)\n",
    "    print('Target:', t)\n",
    "    print('Output:', o)\n",
    "    print('-=-' * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log:\n",
    "\n",
    "### 两位数时：\n",
    "\n",
    "**1**\n",
    "\n",
    "* Cell: BasicRNNCell\n",
    "* dataset size: 20000\n",
    "* Epochs: 5000\n",
    "* Learning_rate: 0.001\n",
    "* Train/Test Acc: 0.997/0.986\n",
    "\n",
    "### 三位数时：\n",
    "\n",
    "**1**\n",
    "\n",
    "* Cell: BasicRNNCell\n",
    "* dataset size: 100000\n",
    "* Epochs: 50000\n",
    "* Learning_rate: 0.001\n",
    "* Train/Test Acc: 0.344/0.322\n",
    "\n",
    "**2**\n",
    "\n",
    "* Cell: BasicRNNCell\n",
    "* dataset size: 200000\n",
    "* Epochs: 5000\n",
    "* Learning_rate: 0.01\n",
    "* Train/Test Acc: 0.005/0.004\n",
    "* Final cost: 1.3729\n",
    "\n",
    "**3**\n",
    "\n",
    "* Cell: BasicRNNCell\n",
    "* dataset size: 200000\n",
    "* Epochs: 50000\n",
    "* Learning_rate: 0.01\n",
    "* Train/Test Acc: 0.001/0.001\n",
    "* Final cost: 1.6427\n",
    "* note: 单纯的加大步数和学习率，感觉没用，反而 cost 有所上升\n",
    "\n",
    "**4**\n",
    "\n",
    "* Cell: BasicRNNCell\n",
    "* dataset size: 2000000\n",
    "* Epochs: 50000\n",
    "* Learning_rate: 0.001\n",
    "* Train/Test Acc: 0.346/0.344\n",
    "* Final cost: 0.3691\n",
    "* note: 加大了数据集和降低了学习率，cost 明显下来了，应该是学习率的问题，准确率也有较大提升\n",
    "\n",
    "**5**\n",
    "\n",
    "* Cell: BasicRNNCell\n",
    "* dataset size: 2000000\n",
    "* Epochs: 50000\n",
    "* Learning_rate: 0.003\n",
    "* Train/Test Acc: 0.982/0.982\n",
    "* Final cost: 0.0941\n",
    "* note: 调整了学习率之后，cost 下降的比较好，最后的结果也不错\n",
    "\n",
    "**6**\n",
    "\n",
    "* Cell: BasicRNNCell\n",
    "* dataset size: 500000\n",
    "* Epochs: 20001\n",
    "* Learning_rate: 0.003\n",
    "* Train/Test Acc: 0.941/0.940\n",
    "* Final cost: 0.0394\n",
    "* note: 试着减少了数据量(现实中数据量很难有第5次那么大)，和迭代次数，发现结果也还不错"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
