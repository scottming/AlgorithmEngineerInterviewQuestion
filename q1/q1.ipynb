{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scott Ming 2017-05-30 \n",
      "\n",
      "CPython 3.6.0\n",
      "IPython 6.0.0\n",
      "\n",
      "numpy 1.12.1\n",
      "\n",
      "compiler   : GCC 4.9.2\n",
      "system     : Linux\n",
      "release    : 3.16.0-4-amd64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Scott Ming' -v -m -d -p numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试题 1\n",
    "\n",
    "在多元线性回归问题中，假设在各个x和y之间存在如下的线性关系：\n",
    "y = w0 + w1 * x1 + w2 * x2 + w3 * x3\n",
    "需要去估计各自变量的权重系数w。请用 Python 写出一段代码，来计算出权重。\n",
    "要求：\n",
    "- 数据采用附件中的data.csv\n",
    "- 工具使用 Python 以及 NumPy 库，不能使用其它扩展库\n",
    "- 写出三种不同的算法\n",
    "    + 一种是使用有显式解析解的矩阵方法\n",
    "    + 一种是使用牛顿法\n",
    "    + 一种是使用随机梯度下降算法\n",
    "- 分别检验不同算法的模型参数在显著水平为 0.05 时的效果\n",
    "- 需要有简单的说明文档和代码注释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st  # 仅用来计算 p- 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt('data.csv', delimiter=',', skip_header=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\beta = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$SSE = \\sum_{i=1}^N(y_i - \\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma^2 = MSE = \\frac{SSE}{n - p - 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C = cov(\\beta) = \\sigma^2(XX^T)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$SE = \\sqrt{C}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$t_i = \\frac{\\beta_i}{SE_{i,i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_p(X, y, w_, predict):\n",
    "    dof = X.shape[0] - X.shape[1]  # Degree of freedom\n",
    "    sse = np.sum((predict - y) ** 2, axis=0)\n",
    "    mse = sse / dof\n",
    "    c = mse * np.linalg.inv(X.T.dot(X))\n",
    "    se = np.sqrt(np.diag(c))  # 取对角线的元素\n",
    "    t = w_ / se\n",
    "    p_ = 2 * (1 - st.t.cdf(np.abs(t), dof))\n",
    "    return t, p_\n",
    "\n",
    "\n",
    "def print_p_values(p_):\n",
    "    x = 'b0 b1 b2 b3'.split()\n",
    "    for x, p in zip(x, p_):\n",
    "        print(f'{x}: {p:.20f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegressionLS(object):\n",
    "    def fit(self, X, y):\n",
    "        self.w_ = np.linalg.lstsq(X, y)[0]\n",
    "        self.t_, self.p_ = compute_p(X, y, self.w_, self.predict(X))\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X[:, 1:], self.w_[1:]) + self.w_[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.net_input(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[:, :3]\n",
    "X = np.c_[np.ones(X.shape[0]).T, X]  # 添加一个截距项在左边\n",
    "y = data[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.03076198,  2.97396653, -0.54139002,  0.97132913])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = LinearRegressionLS()\n",
    "ls.fit(X, y)\n",
    "ls.w_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**显著性检验：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_0 :\\beta_i = 0$$\n",
    "$$H_a :\\beta_i \\neq 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0: 0.00000000000000000000\n",
      "b1: 0.00000000000000000000\n",
      "b2: 0.00000001179840070087\n",
      "b3: 0.00000000000000000000\n"
     ]
    }
   ],
   "source": [
    "print_p_values(ls.p_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的打印结果，可以看出，各 $\\beta$ 的 $p-$ 值远小于 0.05，于是拒绝原假设，$x_i$ 与 $y$ 有显著关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考 PRML 4.3.3 节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$w^{(new)} = w^{(old)} - H^{-1}\\bigtriangledown E(w) \\tag{1.2.1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bigtriangledown E(w) = \\sum_{n=1}^{N}(w^T\\phi_n - t_n)\\phi_n = \\Phi^T\\Phi w - \\Phi^Tt \\tag{1.2.2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H = \\bigtriangledown \\bigtriangledown E(w) = \\sum_{n=1}^{N}\\phi_n\\phi_n^T = \\Phi^T\\Phi \\tag{1.2.3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "w^{(new)} &= w^{(old)} - (\\Phi^T\\Phi)^{-1}\\{\\Phi^T\\Phi w^{(old)} -\\Phi^Tt \\} \\\\\n",
    "&= (\\Phi^T\\Phi)^{-1}\\Phi^Tt \\tag{1.2.4}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面公式可以看出，w 迭代一次就会变成最小二乘解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegressionNR(object):\n",
    "    \"\"\"Newton-Raphoson Method.\"\"\"\n",
    "    def __init__(self, w=[0, 0, 0, 0]):\n",
    "        self.w = w\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Iterate one step \n",
    "        self.w = self.w - np.linalg.inv(X.T.dot(X)).dot(\n",
    "            X.T.dot(X).dot(self.w) - X.T.dot(y))\n",
    "        self.t_, self.p_ = compute_p(X, y, self.w, self.predict(X))\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X[:, 1:], self.w[1:]) + self.w[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.net_input(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr = LinearRegressionNR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里 `w` 因为被 `__init__` 初始化，所以后面不加 `_` 线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化的 w\n",
    "nr.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代一次，拟合之后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.03076198,  2.97396653, -0.54139002,  0.97132913])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.fit(X, y)\n",
    "nr.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0: 0.00000000000000000000\n",
      "b1: 0.00000000000000000000\n",
      "b2: 0.00000001179840070087\n",
      "b3: 0.00000000000000000000\n"
     ]
    }
   ],
   "source": [
    "print_p_values(nr.p_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**显著性检验:** 因为结果与最小二乘法是完全一样的，所以检验结果也一样，不再重复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 随机梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机梯度下降，我在 DL101 课程上有多次实作过，用 Numpy 实作其实也很简单，理解最小化成本函数和更新规则即可，最近刚好在读 Python Machine Learning 这本书，ch2 的关于 **AdLine 分类模型**的代码写的非常漂亮，而线性回归和 AdLine 两者的成本函数其实是完全一样的，两者都是用误差平放和(SSE) 做优化，所以我直接改了这个类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(w) = \\frac{1}{2}\\sum_i(y^{(i)} - \\phi{(z^{(i)})})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\delta J}{\\delta w_j} = \\frac{\\delta}{\\delta w_j}\\frac{1}{2}(y^{(i)} - \\phi{(z^{(i)})})^2 = -\\sum_i(y^{(i)} - \\phi(z^{i}))x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Delta w = -\\eta\\frac{\\delta J}{\\delta w_j}  = \\eta\\sum_i(y^{(i)} - \\phi{(z^{(i)})})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w := w + \\Delta w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegressionSGD(object):\n",
    "    \"\"\"Linear Regression.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Passes over the training dataset.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting.\n",
    "    cost_ : list\n",
    "        Sum-of-squares cost function value averaged over all\n",
    "        training samples in each epoch.\n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent cycles.\n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=20, shuffle=True, random_state=None):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.w_initialized = False\n",
    "        self.shuffle = shuffle\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.cost_ = []\n",
    "        for i in range(self.n_iter):\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            cost = []\n",
    "            # 每个迭代下，打乱数据，然后把每个 xi, target 依次迭代一遍\n",
    "            for xi, target in zip(X, y):\n",
    "                cost.append(self._update_weights(xi, target))\n",
    "            avg_cost = sum(cost) / len(y)\n",
    "            self.cost_.append(avg_cost)\n",
    "        self.t_, self.p_ = compute_p(X, y, self.w_, self.predict(X))\n",
    "        return self\n",
    "\n",
    "    def _shuffle(self, X, y):\n",
    "        \"\"\"Shuffle training data\"\"\"\n",
    "        r = np.random.permutation(len(y))\n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def _initialize_weights(self, m):\n",
    "        \"\"\"Initialize weights to zeros\"\"\"\n",
    "        self.w_ = np.zeros(m)\n",
    "        self.w_initialized = True\n",
    "        \n",
    "    def _update_weights(self, xi, target):\n",
    "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
    "        output = self.net_input(xi)\n",
    "        error = (target - output)\n",
    "        self.w_ += self.eta * xi.dot(error)\n",
    "        # 已经添加了截距项，所以不需要另外求 w_[0]\n",
    "#         self.w_[0] += self.eta * error\n",
    "        cost = 0.5 * error**2\n",
    "        return cost\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.net_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = LinearRegressionSGD(n_iter=50, eta=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.95317769,  2.90773368, -0.52167386,  0.90497183])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(X, y)\n",
    "sgd.w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2948359275842689,\n",
       " 1.9363849020929147,\n",
       " 1.9266239612428337,\n",
       " 1.9274290834534784,\n",
       " 1.9272601525159454,\n",
       " 1.9354518909254301,\n",
       " 1.9369892657599501,\n",
       " 1.9374456223217564,\n",
       " 1.9343635788284617,\n",
       " 1.9212550680188527,\n",
       " 1.9362110158731707,\n",
       " 1.9398331678158576,\n",
       " 1.9345545865661109,\n",
       " 1.9341039214781384,\n",
       " 1.9373954829761602,\n",
       " 1.920753117075195,\n",
       " 1.9267799280903986,\n",
       " 1.9302494972024082,\n",
       " 1.9274435556714449,\n",
       " 1.9271440823503754,\n",
       " 1.9338529693466362,\n",
       " 1.9309339369588017,\n",
       " 1.9351922967151389,\n",
       " 1.9288950460647132,\n",
       " 1.9369090373622002,\n",
       " 1.9354932530540114,\n",
       " 1.9325468439884077,\n",
       " 1.9368158325949341,\n",
       " 1.9401035187864792,\n",
       " 1.9230762752870858,\n",
       " 1.9307334622416124,\n",
       " 1.9334926272473543,\n",
       " 1.9345203043383112,\n",
       " 1.9304455512762488,\n",
       " 1.9324758880899173,\n",
       " 1.9360113462019093,\n",
       " 1.9345500484691025,\n",
       " 1.9279644345200377,\n",
       " 1.9303477731425058,\n",
       " 1.9289942847341102,\n",
       " 1.9232761609143567,\n",
       " 1.9338434365116322,\n",
       " 1.9298349161646449,\n",
       " 1.9299474378528692,\n",
       " 1.9143257490960159,\n",
       " 1.9345587169091312,\n",
       " 1.9193866517343212,\n",
       " 1.9389962287588733,\n",
       " 1.9129040541028886,\n",
       " 1.9352113940707742]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.cost_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD 算法，模型在第二步就开始收敛了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**显著性检验：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_0 :\\beta_i = 0$$\n",
    "$$H_a :\\beta_i \\neq 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0: 0.00000000000000000000\n",
      "b1: 0.00000000000000000000\n",
      "b2: 0.00000004052505619967\n",
      "b3: 0.00000000000000000000\n"
     ]
    }
   ],
   "source": [
    "print_p_values(sgd.p_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的打印结果，可以看出，各 $\\beta$ 的 $p-$ 值远小于 0.05，于是拒绝原假设，$x_i$ 都与 $y$ 有显著关系"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
